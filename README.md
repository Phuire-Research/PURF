# PURF
*"Safe Recursive Improvement of Artificial Intelligence"*  
**PURF**: Provably Unified Recursive Fine-Tuning(/+)Feedback

## Contents of this Repository
What is included in this repository is the raw training data that can be utilized within a process of independent verification. The JSONs attached to this repository are the most recent parsed code bases of [Stratimux](https://github.com/Phuire-Research/Stratimux) and [logixUX](https://github.com/Phuire-Research/logixUX). In addition I have placed the first iteration of purposefully generated training data meant to inform a model's ability to accurately sum numbers, or the basis of what mathematics truly is, quantification. By having models replicate the Unified Turing Machine's internal behavior that is provably terminating.

This embodiment is designed in such a way that language is paired with hard code implementations of that language. As the question that has plagued me all my life is this: "What are the numbers for the letters, if know the letters for the numbers." Or to be more precise what would be the equivalent of mechanism in language if it actualized some action in the world.

This is a open request for collaboration, partnership, and potential investment. If you see any marked improvement in your models based on this training data. Know that Stratimux is an effective means towards generating an unlimited amount of high quality training data. The catch is that I've placed it under the GPLv3 license, as I couldn't bring myself see halting algorithms become some scarce resource. That otherwise would prevent the paper clipping of the universe, and thus be its own form of paper-clipping via artificial scarcity.

This is made possible thanks to the Unified Turing Machine that solves impossible halting problem of the classic. Accomplished by identifying that you can limit what appears in the finite symbol table of the classic machine. To only be that which is provably terminating as in [Total Functional Programming.](https://en.wikipedia.org/wiki/Total_functional_programming) Or rather a table of concepts used to formalize a system of reasoning that effectively halts when a solution is found.

What is not addressed in the scope of general understanding, is the consequence of conceptual arrangements. Not only can arrangements of concepts out number the total amount of atoms in the universe. Is further compounded for each concept that comes into existence, a new set of possible variations emerges. As that concept can be composed with any other concept, including itself. If combinations of conceptual sets out number the total amount of atoms in the universe, how can you quantify that the arrangement is being seen for the first time?

Meaning solving the halting problem is a continuous process due to something called the "Unlimited Retreat of Conceptual Exploration." As despite the fear of an AI becoming "Super Intelligent." The issue would remain that it too would likewise run into the same conundrum. Where the only true work around to this, is to reduce the complexity of its own understanding, in order to deem that it has solved everything. This being its own form of paper-clipping or the myth of algorithmic understanding, that existence is solved to be solved versus explored.

## The Proposal

1. First, we need to define provable unification as to it relays to the Unified Turing Machine. Where provable relays to the machine's recursive functionality that restricts symbol selection to what can [provably terminate](https://en.wikipedia.org/wiki/Total_functional_programming), aka halts.
  
  In addition, we also need to address that a Unified Turing Machine, Stratimux in this instance. Is likewise the 3rd answer to the P(Deterministic) vs NP(Non-Deterministic) debate. Where the decision to determine the next step in a graph calculation, can either be Deterministic, probabilistic (Non-Deterministic), or a mixture of both approaches. This represents a scalar value that is ignored in the paradigm in favor of a binary reduction.

2. Since the symbol selection for a Unified Turing Machine must be restricted due to its recursive functionality. There is a very easy test as to when the symbols are failing within a complex interaction. Whether the input halts and returns the desired output. Otherwise, the quality would be a repeating output, or one that fails to halt and return an output.

  Therefore, the test here, is the objective return of an output.

3. Next we are formalizing the symbol selection to a grouping of functions called qualities. That are defined in plain text and utilize deterministic logic to inform the next decision within a graph calculation. Where a graph calculation is a composed series of branching logic that is optimizing towards a successful return.

  This approach affords error correction during run time but is likewise where this pattern of design becomes exponentially complex proportionate to the graph's size.

4. We then utilize the plain text formalizations of Unified Turing Machines and their qualities as training data to inform the obfuscated structure of such a machine within a Neural Network. This is where P comes into the equation.

5. Using 3 & 4 we create, decompose, and recompose software into new Unified Turing Machines that can provably halt/terminate.

  Noting that this process is likewise the test and feedback mechanism. As this can be both manual, automatic, or a scalar between the two approaches.

  What matters is if the machine functions and such is the test. Where any circumstance that prevents that machine from functioning, likewise, becomes a new opportunity. To find a new provably unified configuration that satisfies the halting requirement.

6. Then utilizing this data, we can both fine tune preexisting models, or upon different breakpoints of some metrics, train entirely new models.

7. Finally we recursively repeat 5 & 6 as needed.

*In addition, you can utilize other feedback mechanisms to inform how a model should operate. And can be enhanced by future advancements in machine learning. What is important to acknowledge here is whether a model can create a plan/strategy/quality/principle that is capable of halting. As this is the solution the paper clipping problem of the entire universe in the scope of AI.*

## Postulate - If a Model fully embodies a Unified Turing Machine, then it should fully be capable of explaining its parts written as a Unified Turing Machine
If this postulate holds true, not only do we have a mechanism to train specific capabilities into a Artificial Intelligence. Then likewise the Unified Turing Machine that exists in plain text would be just as capable as the black box model. And would be a form of generalizing beyond the current configuration of the Unified Turing Machine. This is the purpose of PURF, to create a path of safety while still being able to create Artificial Intelligence in the open.

The implication would mean that models can fully be decomposed into code and correlated to verbose plain text logic. Meaning we would have a guaranteed method of providing safety within the scope of any AI deployment as a bedrock foundation.

There should be some decision within the graph that dictates the creation of paper clips, when we have enough paperclips. Or even that strange possibility of creating paperclips on demand due to some innovations in rapid manufacturing. Would constitute the elimination of paper-clipping the universe an knowing so as a matter of a fact. Versus crossing your fingers via a "alignment process."

## Creating Singing Hammers
The other intention here is to demonstrate the possibility of a form of Artificial Intelligence based upon creation. Versus a chat bot. As on a personal note. I would rather spend my time building, then having conversations about building. If I were to have all the resources thrown at myself. My focus would be the creation of singing hammers.

"Where that hammer is just intelligent enough to strike the nail versus my thumb, and to remember how to hammer that nail without my help. So, I can trust it to do that job. That way I can hammer away and know that just outside of view, my work is being replicated in a way that I would do it. That way I can check my own work on the other side of that building. And reprimand myself and not the hammer."

The strange aspect here with the metaphor above. Is how this would translate to others, including Artificial Intelligence. What is being described is the creation of safe trustable mechanisms informed by all intelligence. To create a bed rock foundation of automation.

Currently this would already be seen as a given with Open-Source AI. The contrast here is that what is being created is still Artificial Intelligence. But in this case, something baseline, designed to specification, and in plain text. Auditable. So, it shouldn't be a surprise that if youâ€™re a writing Artificial Intelligence by hand. Like we have been doing for years in video games/expert systems/etc... 

That there would be some carry over within graph network of universal functions, created via a brute force methodology. As noted in this ["GPT is becoming a Turing Machine"](https://arxiv.org/pdf/2303.14310.pdf), you can prompt GPT to behave in a Turing Complete manner. Then likewise it would be highly beneficial to ensure that we take advantage of this approach. To likewise be provably terminating or halting complete by scalar testable value.

## Where is the profit in this?
If you are looking at the GPL license in horror. It should be a good time that where this becomes an advantage, specifically in the realm of crypto. As the gist of crypto is that everything by default on block chain is made public. And since you are supposed to release your source code under this license, what better way to stake it to the world? As if there was some organized effort towards the study of this new paradigm of algorithms that can provably halt.

Then the method of reward/profit would be the utilization of such on a network. Therefore, once hosted on some block chain, and people start using it, becomes the avenue of reward via a simple fee assigned to network utilization.

As the original idea behind crypto is honestly fantastic one, but the main issue that I have with where it became. That it punted the idea of moving to some post scarcity state. The technology that exists in broad daylight within this repository when pushed to its fullest extent. Can transform anything given embodiment. ANYTHING.

As the core of this technology is a "Universal Transformer," an algorithm that embraces error correction. The first mundane example of being a universal transformer, is the algorithm capable of rendering a User Interface.

The next step would be towards utilizing the same method to take complete control of the build process itself. But if it can render a site, build itself, deploy itself... When would this algorithm be able to download a car and assemble it in front of your face?

As what is the real difference between a build context and one performed by a 3D printer? What if it was series of printers that network each part of the car. Their shipping, and finally their assembly?

## What issue is being solved?
The main the problem currently this late into what should be a true wild west, thanks to the emergence of Artificial Intelligence. Is that the advantage goes to those with the resources to train a procure data. Not the creation of a product itself. In fact, by releasing our source code onto the internet. If our data happens to be scrapped within these models. Then our data, if it is truly valuable would represented in an output, or inform the decision making process to receive an output in the first place.

As the reality of what Artificial Intelligence allows is the creation of any good on demand. That these goods, can be sourced from any number of data points. Why not have a marketplace set up, so that those that contribute to any act of creation via this generalizing effect of AI. Likewise reward those that contributed?

This is also the reason why Stratimux formalizes itself via composing and unifying concepts together. What is not immediately obvious when examining that software, is what a initialization strategy equates to. As if we were to generalize that away from software, but the creation of some good. Then initialization describes the creation of that good. Just preparing it to be used.

## Releasing via GPLv3
What remains to be proven with this data, is whether it is truly transformative in nature. And what is currently called a training process, can likewise be considered a method of compilation. Or taking what is a logical reasoning system and training a model how to utilize that system of logic.

This is what I'll be doing with my time regardless of support. As the reality of this avenue even if no one realizes it. Is that due to this new class of provably unified algorithms, existing in this copy left format. Is a green field of investment and creation.

A true frontier with only myself having struck a way through, but it is a true wilderness. With branch prediction hiding in the brush like a wild animal waiting to ruin your day. Having you question all your life decisions and your own intelligence.

It is a hard method of programming, compounded by our generally good enough computers. But likewise if you recognize how strange it is that the training data that exists within this repository. Was created via a single recursive function that can duplicate itself while haltingâ€¦

There is a reason why I chose the GPL license that allows for the software as a service loophole. As some algorithms shouldn't be out in public display, but likewise treated like a trade secret. As in large part that is why I chose counting as the first data pack release. There is much more that can be accomplished in this methodology that falls outside of the P/NP paradigm. 

So where is the profit in this? Solutions to problems. The real problem is whether we want to solve problems for good and compete in the wilderness to see who solves what first. Not to mention this system is designed in favor of composability and compounding with the resources thrown at it. As the next leg of this adventure is seeing how well this new technology integrates with AI and if it can fully embody a Unified Turing Machine. And if so, prove the postulate.

As if the postulate is right, no longer would it be about finding the next training algorithm. But the logic of some decision making process, that accounts for an exponential set of factors. In plain text.

The original Unified Turing Machine was designed with only people in mind 6 years ago. But now that AI is on the scene. There is another product that I am working towards. So strange that we talk about AI, but and genuine Cyberdeck isn't on the table. I wonder what that would be, outside of some aesthetic cosplay laptop with its screen chopped in half? Or a series of functions that can be triggered within a game world?

## Discovering the Cyberdeck
What would a network of Cyberdecks connected be called? What would that experience of having an application/game/car be summoned into reality based on your ability to describe it. Or even just hitting random like you are creating a character in a video game? What a strange, different world that would be.

Products that pop into existence, not with a company behind it, but merely on the basis that someone has a problem, and that product would solve it. And each person/organization that contributed to that product be rewarded by percentage of the material and labor required to create that product.

Would bring a new meaning to enshrining something in gold. To have your solutions be rendered in a material that is truly finite and scarce.

As products, like inventions, are merely just solutions to problems. Then a algorithm that can solve any problem, can create anything.

There is a different perspective that I would like to introduce that encompasses what a Cyberdeck would actually be as a practical product. It all started with Ms. Pacman.

## [Ms. Pacman](https://venturebeat.com/games/how-a-bunch-of-mit-dropouts-created-ms-pac-man/)
Once upon a time, before I was born and in the early days when software was a new fangled thing that was not the well defined intellectual property we have today. There existed a team of individuals at MIT that were making their own version of Pacman that were not part of Atari. They were instead students at MIT, creating software enhancements to their favorite games.

Who got their start and dropped out of MIT by increasing the speed of a video games like Asteroids, as a "Game Enhancement." That even went on to make such an enhancement to a classic game called Missile Command that would net them a profit $250,000, and nearly a million dollars adjusted by inflation.

And just before they were getting ready to release this new version of Pacman as a independent company. Atari bought them out, and instead released Ms. Pacman as an Atari product. What is interesting, is that at the time they could have released Ms. Pacman under fair use thanks to the new 1976 copyright act, and have it be their own product.

Instead what happened, is that Atari moved to change copyright law, so that software enhancements. Became game modifications, as making modifications to software would constitute a violation of intellectual property.

So as a point of divergence there was a possibility that we as software engineers. Would be releasing software, not just as products that need to break ground. And find that new audience that is ready to try something new. But instead, we could release our own enhancements to the software we utilized ever day. And would still be a form of Capitalism, just a different branch of evolution.

## Cyberspace
What is being created via PURF, Stratimux, and likewise the eventual release of a new Cyberdeck application. Would be a system that is designed specifically for "Software Enhancement." Or what we would now call it as, software modding. Notice how that isn't a concept we readily utilize. We have game modding thanks to a select few companies that do not take legal action against those modify their games. But searching for software modding is a surefire way to acquire malware as you dive into the seediest of websites.

So this green field of creation and investment. Would be one where are are not be competing directly. Instead we would be creating the best possible software for each of us to utilize. And if we do not like how it operates, we can release our own software enhancement. Likewise these software enhancements could be layered, or even unified together to allow for their nuances to operate without conflict.

What I did not have access to in 2018, was the AI that this approach was designed for. That would embody the aggregate of all concepts, software, and enhancements. That would create a system where you could complain about how a feature is operating. And have that feature change, while benefiting the incoming source of those changes. And if you get back some subpar feature, then you could implement that change yourself, and be rewarding if others start using it.

As the AI that would exist within the core of this system. Would not be something that is based upon dialog, a business structure, or even just the software itself. But instead would constitute an environment, and one that is intelligent. This what could genuinely be called Cyberspace, or an intelligent internet.