# PURF
*"Safe Recursive Improvement of Artificial Intelligence"*  
**PURF**: Provably Unified Recursive Feedback

## Contents of this Repository
What is included in this repository is the raw training data that can be utilized within a process of independent verification. The JSONs attached to this repository are the most recent parsed code bases of [Stratimux](https://github.com/Phuire-Research/Stratimux) and [logixUX](https://github.com/Phuire-Research/logixUX). In addition to purposefully generated training data meant to inform a model's ability to halt. By having models replicate the Unified Turing Machine's internal behavior that is provably terminating.

This embodiment is designed in such a way that language is paired with hard code implementations of that language. As the question that has plagued me all my life is this: "What are the numbers for the letters, if know the letters for the numbers." Or to be more precise what would be the equivalent of mechanism in language if it mechanized some action in the world.

This is a request for collaboration, partnership, and potential investment. If you see any marked improvement in your models based on this training data. Know that Stratimux is an effective means towards generating an unlimited amount of training data. The catch is that I've placed it under the GPLv3 license, as I couldn't bring myself see halting algorithms become some scares resource. That otherwise would prevent the paper clipping of the universe.

This is the solution to the impossible halting problem. A pursuit of seeing such as a problem to be solved.

## The Proposal

1. First, we need to define provable unification as to it relays to the Unified Turing Machine. Where provable relays to the machine's recursive functionality that restricts symbol selection to what can [provably terminate](https://en.wikipedia.org/wiki/Total_functional_programming), aka halts.
  
  In addition, we also need to address that a Unified Turing Machine, Stratimux in this instance. Is likewise the 3rd answer to the P(Deterministic) vs NP(Non-Deterministic) debate. Where the decision to determine the next step in a graph calculation, can either be Deterministic, probabilistic (Non-Deterministic), or a mixture of both approaches. This represents a scalar value that is ignored in the paradigm in favor of a binary reduction.

2. Since the symbol selection for a Unified Turing Machine must be restricted due to its recursive functionality. There is a very easy test as to when the symbols are failing within a complex interaction. Whether the input halts and returns the desired output. Otherwise, the quality would be a repeating output, or one that fails to halt and return an output.

  Therefore, the test here, is the objective return of an output.

3. Next we are formalizing the symbol selection to a grouping of functions called qualities. That are defined in plain text and utilize deterministic logic to inform the next decision within a graph calculation. Where a graph calculation is a composed series of branching logic that is optimizing towards a successful return.

  This approach affords error correction during run time but is likewise where this pattern of design becomes exponentially complex proportionate to the graph's size.

4. We then utilize the plain text formalizations of Unified Turing Machines and their qualities as training data to inform the obfuscated structure of such a machine within a Neural Network. This is where P comes into the equation.

5. Using 3 & 4 we create, decompose, and recompose software into new Unified Turing Machines that can provably halt/terminate.

  Noting that this process is likewise the test and feedback mechanism. As this can be both manual, automatic, or a scalar between the two approaches.

  What matters is if the machine functions and such is the test. Where any circumstance that prevents that machine from functioning, likewise, becomes a new opportunity. To find a new provably unified configuration that satisfies the halting requirement.

6. Then utilizing this data, we can both fine tune preexisting models, or upon different breakpoints of some metrics, train entirely new models.

7. Finally we recursively repeat 5 & 6 as needed.

*In addition, you can utilize other feedback mechanisms to inform how a model should operate. And can be enhanced by future advancements in machine learning. What is important to acknowledge here is whether a model can create a plan/strategy/quality/principle that is capable of halting. As this is the solution the paper clipping problem of the entire universe in the scope of AI.*

## Postulate - If a Model fully embodies a Unified Turing Machine, then it should fully be capable of explaining its parts written as a Unified Turing Machine
If this postulate holds true, not only do we have a mechanism to train specific capabilities into a Artificial Intelligence. Then likewise the Unified Turing Machine that exists in plain text would be just as capable as the black box model. And would be a form of generalizing beyond the current configuration of the Unified Turing Machine. This is the purpose of PURF, to create a path of safety while still being able to create Artificial Intelligence in the open.

The implication would mean that models can fully be decomposed into code and correlated to verbose plain text logic. Meaning we would have a guaranteed method of providing safety within the scope of any AI deployment as a bedrock foundation.

There should be some decision within the graph that dictates the creation of paper clips, when we have enough paperclips. Or even that strange possibility of creating paperclips on demand due to some innovations in rapid manufacturing. Would constitute the elimination of paper-clipping the universe an knowing so as a matter of a fact. Versus crossing your fingers via a "alignment process."

## Creating Singing Hammers
The other intention here is to demonstrate the possibility of a form of Artificial Intelligence based upon creation. Versus a chat bot. As on a personal note. I would rather spend my time building, then having conversations about building. If I were to have all the resources thrown at myself. My focus would be the creation of singing hammers.

"Where that hammer is just intelligent enough to strike the nail versus my thumb, and to remember how to hammer that nail without my help. So, I can trust it to do that job. That way I can hammer away and know that just outside of view, my work is being replicated in a way that I would do it. That way I can check my own work on the other side of that building. And reprimand myself and not the hammer."

The strange aspect here with the metaphor above. Is how this would translate to others, including Artificial Intelligence. What is being described is the creation of safe trustable mechanisms informed by all intelligence. To create a bed rock foundation of automation.

Currently this would already be seen as a given with Open-Source AI. The contrast here is that what is being created is still Artificial Intelligence. But in this case, something baseline, designed to specification, and in plain text. Auditable. So, it shouldn't be a surprise that if youâ€™re a writing Artificial Intelligence by hand. Like we have been doing for years in video games/expert systems/etc... 

That there would be some carry over within graph network of universal functions, created via a brute force methodology. As noted in this ["GPT is becoming a Turing Machine"](https://arxiv.org/pdf/2303.14310.pdf), you can prompt GPT to behave in a Turing Complete manner. Then likewise it would be highly beneficial to ensure that we take advantage of this approach. To likewise be provably terminating or halting complete by scalar testable value.

## How could there be any profit in this?
If you are looking at the GPL license in horror. It should be a good time that where this becomes an advantage, specifically in the realm of crypto. As the gist of crypto is that everything by default on block chain is made public. And since you are supposed to release your source code under this license, what better way to stake it to the world? As if there was some organized effort towards the study of this new paradigm of algorithms that can provably halt.

Then the method of reward/profit would be the utilization of such on a network. Therefore, once hosted on some block chain, and people start using it, becomes the avenue of reward via a simple fee assigned to network utilization.

As the original idea behind crypto is honestly fantastic one, but the main issue that I have with where it became. That it punted the idea of moving to some post scarcity state. The technology that exists in broad daylight within this repository when pushed to its fullest extent. Can transform anything given embodiment. ANYTHING.

As the core of this technology is a "Universal Transformer," an algorithm that embraces error correction. Just like I am moving to some other plan after ramming my head into the wall to see if I can fine tune a model on the same day that all my work is supposed to be do after a months time.

Mind you this past month I managed to take this universal transformer to create an entire application from scratch. What I started with was the ability to render some template html onto the browser statically. And now its fully reactive, generating pages dynamically, and writing the disk. Proving a recursively halting algorithm that is capable of being Turing Complete.

Now I already see a route towards utilizing the same method to take complete control of the build process itself. When would this algorithm be able to download a car and assemble it in front of your face?

This is what I'll be doing with my time regardless of support. As the reality of this avenue even if no one realizes it. Is that due to this new class of algorithm existing in this copy left format. Is a green field of investment and creation.

A true frontier with only myself having struck a way through, but it is a true wilderness. With branch prediction hiding in the brush like a wild animal waiting to ruin your day. Having you question all your life decisions and your own intelligence. It is a hard method of programming, compounded by our generally good enough computers. But likewise if you recognize how strange it is that the training data that exists within this repository. Was created via a single recursive function that can duplicate itself while haltingâ€¦

There is a reason why I chose the GPL license that allows for the software as a service loophole. As some algorithms shouldn't be out in public display, but likewise treated like a trade secret. As in large part that is why I chose counting as the first data pack release. There is much more that can be accomplished in this methodology that falls outside of the P/NP paradigm. And training an AI on that data, would be a strange situation.

So where is the profit in this? Solutions to problems. The real problem is whether we want to solve problems for good and compete in the wilderness to see who solves what first. Not to mention this system is designed in favor of composability and compounding with the resources thrown at it. As the next leg of this adventure is seeing how well this new technology integrates with AI and if it can fully embody a Unified Turing Machine. And if so, prove the postulate.

As if the postulate is right, no longer would it be about finding the next training algorithm. But the logic of some decision making process, that accounts for an exponential set of factors. In plain text.

The original Unified Turing Machine was designed with only people in mind 6 years ago. But now that AI is on the scene. There is another product that I am working towards. So strange that we talk about AI, but and genuine Cyberdeck isn't on the table. I wonder what that would be, outside of some aesthetic cosplay laptop with its screen chopped in half?

What would a network of Cyberdecks connected be called? What would that experience of having an application/game/car be summoned into reality based on your ability to describe it. Or even just hitting random like you are creating a character in a video game? What a strange, different world that would be. Products that pop into existence, not with a company behind it, but merely on the basis that someone has a problem, and that product would solve it.

As products, like inventions, are merely just solutions to problems. Then a algorithm that can solve any problem, can create anything.